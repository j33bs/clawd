# vLLM OpenAI-compatible server (System-1 local)
#
# Notes:
# - vLLM requires Linux + CUDA; on Windows use Docker Desktop (WSL2 backend) or run inside WSL2.
# - Model selection is operator-provided via VLLM_MODEL.
# - No secrets are stored here; optional API key gating is controlled by OPENCLAW_VLLM_API_KEY.
#
# References (docs only):
# - https://docs.vllm.ai/en/latest/serving/openai_compatible_server/
# - https://docs.vllm.ai/en/v0.5.1/getting_started/installation.html

services:
  vllm:
    image: ${VLLM_IMAGE:-vllm/vllm-openai:latest}
    container_name: openclaw-vllm
    # vLLM OpenAI-compatible server exposes /v1/* by default.
    command:
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "--model"
      - "${VLLM_MODEL}"
      - "--gpu-memory-utilization"
      - "${VLLM_GPU_MEMORY_UTILIZATION:-0.90}"
      - "--max-model-len"
      - "${VLLM_MAX_MODEL_LEN:-8192}"
    ports:
      - "127.0.0.1:${VLLM_PORT:-8000}:8000"
    # If you use HF models, cache can be persisted on the host.
    volumes:
      # Default points into workspace/.state (gitignored).
      - "${VLLM_MODEL_CACHE:-../../workspace/.state/vllm_cache}:/root/.cache"
    # Prefer the compose-native GPU request if supported.
    gpus: all
    # GPU passthrough (Docker Compose v2+). Requires NVIDIA Container Toolkit.
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
