# Model Routing Audit (20260218T194145Z)

## Phase 0 Preflight

```bash
git status --porcelain -uall
?? jest.config.js
?? workspace/audit/model_routing_20260218T194145Z.md
```

```bash
git rev-parse --short HEAD
1f11547
```

```bash
node -v && npm -v
v22.22.0
10.9.4
```

```bash
python3 -V
Python 3.12.3
```

## Phase 1 Discovery Evidence

```bash
ls -la | rg -n "openclaw\.json" || true
# (no output)
```

```bash
find . -maxdepth 4 -iname "openclaw.json" -o -iname "*policy*.json" -o -iname "*routing*.json"
./workspace/policy/llm_policy.json
```

```bash
rg -n "openclaw\.json|llm_policy|policy_router|ProviderRegistry|routing|allowPaid|preferLocal" -S core workspace scripts tests
# Key hits:
# - workspace/policy/llm_policy.json
# - workspace/scripts/policy_router.py
# - workspace/scripts/verify_llm_policy.sh
# - workspace/scripts/verify_policy_router.sh
```

Decision: repo-root `openclaw.json` is not present. Canonical routing is `workspace/policy/llm_policy.json` + `workspace/scripts/policy_router.py`, so implementation was applied there (no new config surface added).

Backups created before edits:
- `workspace/audit/backups/20260218T194326Z/workspace/policy/llm_policy.json.bak`
- `workspace/audit/backups/20260218T194326Z/workspace/scripts/policy_router.py.bak`
- `workspace/audit/backups/20260218T194326Z/workspace/scripts/verify_policy_router.sh.bak`
- `workspace/audit/backups/20260218T194556Z/workspace/scripts/verify_coding_ladder.sh.bak`

## Phase 2-4 Implementation Summary

Authoritative policy updates (`workspace/policy/llm_policy.json`):
- Added conversation-capable providers:
  - `minimax_m25` -> `minimax-portal/MiniMax-M2.5`
  - `minimax_m25_lightning` -> `minimax-portal/MiniMax-M2.5-Lightning`
  - `openai_gpt52_chat` -> `gpt-5.2-chat-latest`
  - `openai_gpt53_codex` -> `gpt-5.3-codex`
  - `openai_gpt53_codex_spark` -> `gpt-5.3-codex-spark`
- Added `routing.intents.conversation` order with MiniMax M2.5 primary.
- Added `routing.capability_router` config for explicit triggers, subagent default, complexity escalation, and deterministic structure thresholds.
- Added local assistant context capability:
  - `providers.local_vllm_assistant.capabilities.context_window_tokens = 16384`
  - env override key: `LOCAL_ASSISTANT_CONTEXT_WINDOW_TOKENS`

Router updates (`workspace/scripts/policy_router.py`):
- Added deterministic trigger/complexity parser with auditable decisions.
- Implemented precedence (after eligibility checks):
  1. hard gates (`_provider_available`, circuit, budgets)
  2. explicit phrase triggers (primary-only):
     - `use chatgpt` -> `openai_gpt52_chat`
     - `use codex` -> `openai_gpt53_codex`
  3. subagent default -> `local_vllm_assistant`
  4. complexity escalation:
     - reasoning/planning -> `openai_gpt52_chat`
     - coding -> `openai_gpt53_codex`
     - small coding -> `openai_gpt53_codex_spark`
  5. default route order (MiniMax M2.5 first for conversation)
- Added `explain_route(intent, context_metadata, payload)` output with:
  - matched trigger/detail
  - chosen provider+model
  - fallback candidates
  - local context window token capability
- Added evidence-first route log event `router_route_selected` with trigger/reason/fallbacks.
- Added fallback intent behavior: unknown intents inherit `conversation` config when present.

## Phase 5 Tests

Policy verification scripts:

```bash
bash workspace/scripts/verify_llm_policy.sh
ok
```

```bash
bash workspace/scripts/verify_policy_router.sh
ok
```

```bash
bash workspace/scripts/verify_coding_ladder.sh
ok
```

Notes:
- `workspace/scripts/verify_policy_router.sh` extended with deterministic routing assertions:
  - default MiniMax M2.5
  - explicit `use chatgpt`
  - explicit `use codex`
  - subagent -> local assistant
  - reasoning -> gpt-5.2 chat
  - coding -> gpt-5.3 codex
  - small coding -> gpt-5.3 codex-spark
  - explain hook includes trigger + local context capability
- `workspace/scripts/verify_coding_ladder.sh` expected order was updated to match current canonical coding ladder with local-first entries.

Full test suite:

```bash
npm test
# Result: FAILURES: 3/17
# Existing unrelated failures observed in:
# - tests/redact_audit_evidence.test.js (CLI JSON summary parsing)
# - tests/system2_experiment.test.js (CLI exits 3)
# - tests/system2_snapshot_diff.test.js (JSON parse / human output expectations)
```

## Local 16k -> 32k Upgrade Path (Safe)

Current implemented capability:
- Local assistant context window is represented explicitly as `16384` tokens and surfaced in route explanations.
- Override path exists via `LOCAL_ASSISTANT_CONTEXT_WINDOW_TOKENS` env.

To move to 32k safely:
1. Confirm model actually supports 32k context.
2. Run vLLM with `--max-model-len 32768` for local assistant serving process.
3. Validate GPU KV-cache headroom (memory utilization / max-num-seqs) under expected concurrency.
4. Set `LOCAL_ASSISTANT_CONTEXT_WINDOW_TOKENS=32768` (or policy value) only after runtime confirmation.

## Known Limits / Hardening Next

- Complexity detection is deterministic keyword+structure based; no probabilistic classifier is used.
- Explicit triggers intentionally do not apply to subagents unless `explicitApplyToSubagents` is set true.
- `context_window_tokens` currently uses policy/env metadata; no live vLLM metadata probe was added because probing infra is not currently part of this router path.
