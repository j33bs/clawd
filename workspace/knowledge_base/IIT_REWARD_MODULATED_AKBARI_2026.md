# IIT-Inspired Consciousness in LLMs: Reward-Based Learning

**Source:** arXiv:2601.22786 (January 2026)
**Authors:** Hamid Reza Akbari, Mohammad Hossein Sameti, Amir M. Mansourian, Mohammad Hossein Rohban
**Code:** https://github.com/MH-Sameti/LT_PostTraining.git

**Added:** 2026-02-24

## Key Innovation

Implements IIT principles within language models via a **reward-based learning paradigm**:

- IIT provides "a formal, axiom-based mathematical framework for quantifying consciousness"
- Novel reward function quantifies a text's:
  - **Causality**
  - **Coherence**
  - **Integration**

## Empirical Results

- Optimizing for IIT-inspired reward → **more concise text generation**
- Up to **31% reduction in output length** while preserving accuracy
- Analysis of confidence calibration and test-time computational scaling

## Why This Matters

Instead of measuring Φ directly (computationally expensive), this approach:
- Uses reward as a **proxy** for integration
- Trains models to produce more integrated outputs
- Is "conceptually simple, computationally efficient, requires no external data or auxiliary models"

## Relevance to TACTI

This connects directly to our reservoir question:
- **Reward-modulated integration** = our reservoir could learn to optimize for integration
- The 31% output reduction suggests coherence has a measurable cost
- Could inform how our reservoir contributes to response mode (INV-002 showed it's neutral to routing but functional for response mode)

## Quote

> "Consciousness-like processing could serve as a key facilitator" in AGI. Current LLMs "exhibit behaviors analogous to certain aspects of consciousness."

## Reference
- arXiv:2601.22786
