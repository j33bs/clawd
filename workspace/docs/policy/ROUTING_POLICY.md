# Routing Policy (System-1)

This repo treats routing as policy/config (LOAR-aligned): operator intent is tracked and reviewed, while runtime discovery is cached separately.

## Tiers

1. Tier-0 (Default): Local vLLM (OpenAI-compatible) on `http://127.0.0.1:8000/v1`.
2. Tier-1: Free-tier external providers (only if configured and allowed by policy).
3. Tier-2: Paid external providers (disabled by default; allowed only when policy enables `allowPaid`).
4. Tier-3: Rented compute (explicit operator activation only; time-boxed).

## Where The Policy Lives

* Canonical policy file: `workspace/policy/llm_policy.json` (version 2)
* This file defines:
  * budgets (per intent and per tier)
  * provider inventory (type + baseUrl + enabled)
  * routing order per intent
  * circuit breaker defaults
  * a `routing.burst_mode` flag with a TTL (time-boxed escalation)

## Local-First Default

System-1 should prefer local inference.

In `workspace/policy/llm_policy.json`:

* `defaults.preferLocal` is `true`
* `providers.local_vllm.enabled` is `true`
* `routing.free_order` places `local_vllm` first

## Governed Escalation

Escalation is policy-driven and operator-controlled:

* Paid escalation requires policy `allowPaid: true` for the relevant intent.
* Burst mode is explicit:
  * Set `routing.burst_mode.enabled` to `true` only for time-boxed sessions.
  * Keep TTL short (default 900s).

## Model Updates (Discover -> Propose -> Promote)

Runtime discovery should never silently mutate tracked catalogs.

* Observed cache: `workspace/.state/models_observed.json` (untracked)
* Proposal diff: generated by `scripts/models_reconcile.ps1`
* Promotion into tracked intent requires explicit operator action.
