# Handoff

## Snapshot
- Date (UTC): 2026-02-14T01:23:43Z
- Branch: codex/system2-models-sorted
- HEAD: 16e3f6b
- Status:

```text
 M scripts/system2/run_local_vllm.sh
?? docs/HANDOFFS/HANDOFF-20260214-012343.md
```

## Objective
Keep the OpenAI-compatible local API surface stable (`http://127.0.0.1:8000/v1`) while enabling an Apple Silicon (M2) accelerated backend path via `vllm-metal` (MLX/Metal). Preserve the generation-probe gating so `local_vllm` is eligible only when it can actually generate.

## Current Evidence (No Secrets)
- `/health` and `/v1/models` return 200.
- Generation endpoints hang/time out (curl sees `HTTP=000` / 0 bytes).
- System-2 diagnostics correctly mark:
  - `local_vllm_generation_probe_ok=false`
  - `local_vllm: eligible=no reason=generation_probe_failed`

Why `/health` + `/v1/models` is insufficient:
- The HTTP server + model registry can be responsive while the actual decode path is wedged (queue deadlock, backend failure, resource starvation). The generation probe is the only deterministic “can it answer a 1-token request” signal.

## Changes
- Updated local runner script to be backend-aware (no contract changes):
  - `scripts/system2/run_local_vllm.sh`
    - `LOCAL_LLM_BACKEND=vllm_cpu|vllm_metal` (default: `vllm_cpu`)
    - Uses `OPENCLAW_VLLM_MODEL` as the preferred model knob (falls back to `VLLM_MODEL`)
    - Metal path defaults `VLLM_VENV=~/.venv-vllm-metal` and verifies `vllm_metal` is importable before launching.
    - Keeps the server surface stable: `--host 127.0.0.1 --port 8000` (overrideable via env).

## How To Run (Operator)
CPU (existing behavior, conservative knobs):
```bash
LOCAL_LLM_BACKEND=vllm_cpu bash scripts/system2/run_local_vllm.sh
```

Metal (after installing vllm-metal in its venv):
```bash
# Summarized install path (see upstream vllm-metal README for full details):
# curl -fsSL https://raw.githubusercontent.com/vllm-project/vllm-metal/main/install.sh | bash

LOCAL_LLM_BACKEND=vllm_metal \
VLLM_VENV=~/.venv-vllm-metal \
OPENCLAW_VLLM_MODEL=Qwen/Qwen2.5-3B-Instruct \
bash scripts/system2/run_local_vllm.sh
```

Verification (contract stable; generation-probe is the source of truth):
```bash
curl -sS --max-time 3 http://127.0.0.1:8000/v1/models | head -c 200; echo
node scripts/vllm_probe.js --json; echo "exit=$?"
node scripts/system2/provider_diag.js | head -n 200
```

Expected:
- If generation is working, `scripts/vllm_probe.js` exits `0` and `provider_diag` shows `local_vllm: eligible=yes`.
- If generation is wedged, probe exits non-zero and `provider_diag` shows `eligible=no reason=generation_probe_failed`.

## Optional Seam (Docs Only)
Any local server that exposes the same OpenAI-compatible API can be used without changing routing/adapter contracts by only setting:
- `SYSTEM2_VLLM_BASE_URL` (or `OPENCLAW_VLLM_BASE_URL`)

Examples include Ollama or other OpenAI-compatible local shims. This is a configuration seam only (no new adapters).
