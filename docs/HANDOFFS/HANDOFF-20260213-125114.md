# Handoff

## Snapshot
- Date (UTC): 2026-02-13T12:51:14Z
- Branch: codex/index-handoff-paid-fallback
- HEAD: e15cd91
- Status:

```text

```

## Objective
Operate System-2 as human-in-the-loop programming support:
1. Navigation and continuity are first-class (Index + Handoff).
2. Runtime routing stays vLLM-first, then free tiers (Gemini/Qwen/Groq/OpenRouter) as configured.
3. No OpenAI/Codex API automation for now (no API keys; cost not justified).
4. Keep resilience hardening (timeout classification, bounded retries, circuit breakers).

## Progress (Evidence-Based)
- Quarantined untracked repo-root governance files (kept out of git worktree).
- Confirmed System-2 inference stack anchors via `docs/INDEX.md`.
- Made `openai*` providers inert by policy (skipped in router and never instantiated in registry).
- Added `remote_vllm` catalog entry as an explicit paid fallback lane (operator-rented compute), disabled unless base URL is set.
- `npm test` passes.

## Current State
- Index and handoff tooling:
  - `node scripts/index/gen_index.mjs` generates `docs/INDEX.md` and `docs/INDEX.json` (paths only; excludes `docs/HANDOFFS/`).
  - `bash scripts/handoff/new_handoff.sh` creates a new structured handoff under `docs/HANDOFFS/`.
  - `bash scripts/handoff/print_context.sh` prints a safe paste-ready context bundle for new chats.
- System-2 inference (FreeComputeCloud):
  - `local_vllm` remains the preferred path when enabled (`ENABLE_LOCAL_VLLM=1`) and healthy.
  - Free tiers remain available as configured via env vars / allowlist / denylist.
  - `openai` provider remains present but is **disabled by operator policy** (no routing, no probing).
  - `remote_vllm` exists as a paid fallback lane but requires explicit base URL config.

## Next Steps
1. Start each Codex window by pasting:
   - `bash scripts/handoff/print_context.sh`
2. If you rent remote vLLM compute and want it as fallback:
   - set `OPENCLAW_REMOTE_VLLM_BASE_URL` (and optionally `OPENCLAW_REMOTE_VLLM_API_KEY`)
   - keep `ENABLE_FREECOMPUTE_CLOUD=1`
3. If you later decide to re-enable OpenAI API:
   - treat it as a separate governed change; require explicit enable flag and avoid making it default.

## Verification (Commands + Expected)
```bash
# Navigation + continuity
sed -n '1,120p' docs/INDEX.md
bash scripts/handoff/print_context.sh | head -n 120

# Tests
npm test

# Diagnostics (no secrets)
node scripts/system2/provider_diag.js | head -n 80
```

Expected:
- `npm test` prints `OK 15 test group(s)`.
- Diagnostics shows `openai: policy=disabled` and `remote_vllm: configured=no` unless a base URL is set.

## Notes / Risks
- The `remote_vllm` lane is intentionally explicit: no default base URL and no directory/network side effects unless configured and enabled in FreeComputeCloud.
- OpenAI/Codex API automation is intentionally disabled by policy to avoid accidental spend and to keep the system human-in-the-loop.
