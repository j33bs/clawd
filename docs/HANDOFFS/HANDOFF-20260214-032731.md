# Handoff

## Snapshot
- Date (UTC): 2026-02-14T03:27:31Z
- Branch: codex/system2-models-sorted
- HEAD: cd6d2fa
- Status:

```text
 M scripts/system2/run_local_vllm.sh
?? docs/HANDOFFS/HANDOFF-20260214-032731.md
```

## Objective
Get local vLLM on Apple Silicon (Metal/MLX via vllm-metal) to *actually generate tokens* (not just respond to `/health` and `/v1/models`), while keeping the OpenAI-compatible surface stable at `http://127.0.0.1:8000/v1` and preserving OpenClaw’s generation-probe gating (local eligible only when generation succeeds).

## Why /health + /v1/models Is Insufficient
The HTTP server and model registry can be responsive while the decode path is wedged (deadlock, backend init failure, OOM during warmup, etc.). OpenClaw now uses a deterministic generation probe (tiny `/v1/completions` POST with a short timeout) as the only “healthy for routing” signal.

## Evidence (No Secrets)
- A server can be listening on `127.0.0.1:8000` and return `200` for `/health` and `/v1/models`, but generation requests time out (0 bytes / `HTTP=000` via curl).
- `node scripts/system2/provider_diag.js` reflects this correctly:
  - `local_vllm_generation_probe_ok=false` and `local_vllm: eligible=no reason=generation_probe_failed`

## Metal Install Reality: “Official Installer” Still Needs a Coherent Dependency Set
The official vllm-metal installer creates `~/.venv-vllm-metal` and installs vLLM + vllm-metal, but we observed:
- vLLM `0.13.0` fails to import with `transformers==5.x` (`ALLOWED_LAYER_TYPES` import error).
- vllm-metal runtime fails if `mlx-lm` is too new (e.g. missing `MambaCache`), which prevents the EngineCore from starting.

The supported operator intent remains:
1. Install using the official vllm-metal installer.
2. Keep that venv isolated (do not mix other MLX ecosystem packages/projects into it).
3. Verify vLLM server generation via `node scripts/vllm_probe.js --json` (exit code must be 0).

## Changes In This Repo
- Hardened `scripts/system2/run_local_vllm.sh` to:
  - Require the vllm-metal venv for `LOCAL_LLM_BACKEND=vllm_metal`.
  - Force the venv’s `python3` for metal.
  - Add sanity checks for common “official installer produced an inconsistent env” failures (vLLM import, vllm-metal import, `mlx_lm.models.cache.MambaCache` presence).
  - Default metal dtype to `float16` (`VLLM_DTYPE` override supported).

## Hardware Note (8GB Unified Memory)
On an 8GB M2, larger models (e.g. `Qwen/Qwen2.5-3B-Instruct`) can crash during warmup with Metal OOM (`Command buffer execution failed: Insufficient Memory`). A smaller model (e.g. `Qwen/Qwen2.5-0.5B-Instruct`) is a safer starting point to validate end-to-end generation.

## Operator Workflow
Install Metal backend (official installer):
```bash
curl -fsSL https://raw.githubusercontent.com/vllm-project/vllm-metal/main/install.sh | bash
```

Run (CPU vs Metal; API surface stays `127.0.0.1:8000/v1`):
```bash
# Metal:
LOCAL_LLM_BACKEND=vllm_metal VLLM_VENV=~/.venv-vllm-metal OPENCLAW_VLLM_MODEL=Qwen/Qwen2.5-0.5B-Instruct \
bash scripts/system2/run_local_vllm.sh
```

Verify (generation probe is the gate):
```bash
curl -sS --max-time 3 http://127.0.0.1:8000/v1/models | head -c 200; echo
node scripts/vllm_probe.js --json; echo "exit=$?"
node scripts/system2/provider_diag.js | head -n 200
```

Expected success:
- `vllm_probe` exit code `0`
- `local_vllm_generation_probe_ok=true`
- `local_vllm: eligible=yes reason=ok`
