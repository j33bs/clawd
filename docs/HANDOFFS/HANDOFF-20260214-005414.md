# Handoff

## Snapshot
- Date (UTC): 2026-02-14T00:54:14Z
- Branch: codex/system2-models-sorted
- HEAD: 7fe74ad
- Status:

```text
 M docs/INDEX.json
 M docs/INDEX.md
?? docs/HANDOFFS/HANDOFF-20260214-005414.md
?? scripts/system2/run_local_vllm.sh
```

## Objective
Resolve the failure mode where the local vLLM server responds to `/health` and `/v1/models` but generation endpoints (`/v1/chat/completions`, `/v1/completions`) hang/time out, by gating `local_vllm` eligibility on a deterministic generation probe (short timeout). Keep System-2 routing robust:
- Local inference healthy: always available escape hatch.
- Local inference unhealthy: fall back to configured external free tiers when enabled (and only when required auth is configured).
- Never auto-route OpenAI API-key lanes without keys; keep Codex OAuth manual-only.

## Evidence (No Secrets)
Observed state (server is listening and models list works, but generation hangs):
```bash
lsof -nP -iTCP:8000 -sTCP:LISTEN
# shows a Python process listening on 127.0.0.1:8000

ps aux | grep -i "vllm.entrypoints.openai.api_server" | grep -v grep
# shows vLLM started with model Qwen/Qwen2.5-3B-Instruct on port 8000

curl -sS --max-time 10 -w "\nHTTP=%{http_code}\n" \
  -X POST http://127.0.0.1:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"Qwen/Qwen2.5-3B-Instruct","prompt":"OK\n","max_tokens":1,"temperature":0}'
# times out with HTTP=000 (0 bytes)

curl -sS --max-time 10 -w "\nHTTP=%{http_code}\n" \
  -X POST http://127.0.0.1:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"Qwen/Qwen2.5-3B-Instruct","messages":[{"role":"user","content":"OK"}],"max_tokens":1,"temperature":0,"stream":false}'
# times out with HTTP=000 (0 bytes)
```

## Changes
- Added a deterministic OpenAI-compatible generation probe:
  - `core/system2/inference/provider_adapter.js`
    - `ProviderAdapter.generationProbe()` posts to `/v1/completions` with `max_tokens=1` and a short timeout (default 5000ms).
- Gated `local_vllm` routing on generation probe:
  - `core/system2/inference/provider_registry.js`
    - Runs the probe (cached 60s) before routing; marks `local_vllm` unhealthy with reason `generation_probe_failed:<subreason>` when it fails.
- Diagnostics now surface the probe result (no secrets):
  - `scripts/system2/provider_diag.js`
    - Prints `local_vllm_generation_probe_ok` and `local_vllm_generation_probe_reason`.
    - Marks `local_vllm` `eligible=no reason=generation_probe_failed` when probe fails.
  - `core/system2/inference/vllm_provider.js`, `scripts/vllm_probe.js`
    - Probe output includes `generation_probe_ok` and `generation_probe_reason`.
- Tests (mocked, no network):
  - `tests/freecompute_cloud.test.js`
    - If local probe fails, registry skips local and falls back to a configured external adapter.
    - If local probe passes, local remains preferred.

## Operator Knobs
- Probe timeout (ms): `FREECOMPUTE_LOCAL_VLLM_PROBE_TIMEOUT_MS` (default: `5000`)
  - Increase if local generation is slow but working.
  - Keep low if you want fast wedge detection.

## Operator Run Script
- `scripts/system2/run_local_vllm.sh`
  - Conservative defaults for 8GB Macs (CPU target, short context, no swap).
  - Override via env: `VLLM_HOST`, `VLLM_PORT`, `VLLM_MODEL`, `VLLM_MAX_MODEL_LEN`, etc.

## Verification
```bash
# 1) Safe diagnostics (no secrets)
node scripts/system2/provider_diag.js | head -n 200
# Expect in wedged state:
# - local_vllm_endpoint_present=true
# - local_vllm_models_fetch_ok=true
# - local_vllm_generation_probe_ok=false
# - local_vllm: eligible=no reason=generation_probe_failed

# 2) Deterministic vLLM probe (includes generation probe)
node scripts/vllm_probe.js --json
# Expect generation_probe_ok=false (and exit code non-zero) in wedged state

# 3) If you configure external keys + enable cloud/free tiers, local unhealthy should fall back:
ENABLE_FREECOMPUTE=1 node scripts/system2/provider_diag.js | head -n 200

# 4) Run local server (operator-invoked)
scripts/system2/run_local_vllm.sh
```

## Notes / Risks
- False negatives: on very slow local inference, a 5s generation probe may time out even if generation would succeed with a longer wait. If that happens, raise `FREECOMPUTE_LOCAL_VLLM_PROBE_TIMEOUT_MS`.
- This change is routing/health-policy only (LOAR-aligned); no data formats or external contracts were changed.
