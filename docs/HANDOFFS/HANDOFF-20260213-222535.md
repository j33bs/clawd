# Handoff

## Snapshot
- Date (UTC): 2026-02-13T22:25:35Z
- Branch: codex/system2-models-sorted
- HEAD: 1ef3495
- Status:

```text

```

## Objective
Stop embedded/agent routing failures of the form "All models failed (...)" by making local vLLM the non-optional escape hatch when healthy, while ensuring cloud/free providers are only attempted when enabled + configured, and OpenAI API-key providers are never attempted without keys (Codex OAuth remains out of automated fallback).

## Progress (Evidence-Based)
- FreeComputeCloud config:
  - Added env alias: `ENABLE_FREECOMPUTE=1` now enables FreeComputeCloud alongside canonical `ENABLE_FREECOMPUTE_CLOUD=1`.
  - Made local vLLM default-enabled (disable only via `ENABLE_LOCAL_VLLM=0`).
- Routing/dispatch hardenings:
  - Router now accepts an `availableProviderIds` set so it will not emit candidates for providers that have no adapter (e.g. missing API keys).
  - Router injects an `escape_hatch_local_vllm` candidate if the scored list would be empty but local vLLM is available+healthy (prevents "no candidates" when local is up).
  - ProviderAdapter no longer calls OpenAI-compatible endpoints with `model=AUTO_DISCOVER`; it resolves a real model ID via env (`OPENCLAW_VLLM_MODEL`) or `/v1/models` discovery.
- Local vLLM defaults:
  - Default System-2 vLLM base URL changed to `http://127.0.0.1:8000/v1` (override via `SYSTEM2_VLLM_BASE_URL` / `OPENCLAW_VLLM_BASE_URL`).
- Diagnostics:
  - `node scripts/system2/provider_diag.js` now reports `configured/enabled/eligible/reason` per provider and shows which freecompute enable keys are present (names only).
- Tests:
  - Added deterministic coverage for `ENABLE_FREECOMPUTE` alias.
  - Added deterministic registry dispatch test that forces success via `local_vllm` without network.

## Current State
- Default behavior (when FreeComputeCloud is enabled) is now "local_vllm first", then only providers that are both configured and allowed by policy/allowlist/denylist.
- OpenAI API-key provider remains policy-disabled in this repo's FreeComputeCloud router/registry (`openai` is not part of automated fallback).
- Note: local vLLM health probe currently reports unreachable in this shell session (no listener on `127.0.0.1:8000` observed during verification). If vLLM is expected to be running, restart it and re-run verification below.

## Next Steps
1. Ensure local vLLM is running and reachable at `http://127.0.0.1:8000/v1`.
2. Export freecompute enable flag (`ENABLE_FREECOMPUTE=1` or `ENABLE_FREECOMPUTE_CLOUD=1`) and verify diagnostics show `local_vllm: eligible=yes` and `local_vllm_endpoint_present=true`.
3. If you want any external free-tier providers, add the appropriate `OPENCLAW_*_API_KEY` env vars (or enable `ENABLE_SECRETS_BRIDGE=1`), and optionally set `FREECOMPUTE_PROVIDER_ALLOWLIST`.

## Verification (Commands + Expected)
```bash
# 1) Confirm local vLLM endpoint (no secrets)
curl -sS -o /dev/null -w "%{http_code}\n" --max-time 3 http://127.0.0.1:8000/v1/models
# Expect: 200

# 2) Provider diag (no secrets)
ENABLE_FREECOMPUTE=1 node scripts/system2/provider_diag.js | head -n 120
# Expect (when vLLM is up):
# - freecompute_enabled=true
# - freecompute_env_keys_seen includes ENABLE_FREECOMPUTE (or ENABLE_FREECOMPUTE_CLOUD)
# - local_vllm_endpoint_present=true
# - local_vllm_models_fetch_ok=true
# - local_vllm: eligible=yes
# - openai: eligible=no (policy-disabled; never auto-routed)

# 3) Tests
npm test
```

## Notes / Risks
- The FreeComputeCloud router is still globally gated by `ENABLE_FREECOMPUTE_CLOUD=1` (or alias `ENABLE_FREECOMPUTE=1`). If neither is set, this module will not dispatch.
- Diagnostics intentionally do not print URLs or secret values; only key names and booleans/counters.
